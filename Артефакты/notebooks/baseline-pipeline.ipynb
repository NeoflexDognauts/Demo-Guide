{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88427f46-941d-4707-97aa-d0aa5b9b1ab3",
   "metadata": {},
   "source": [
    "### **PIPELINE ДЛЯ BASELINE ДАННЫХ (TRAIN.CSV)**  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7a52e-54c6-4485-9f85-2c21193de30a",
   "metadata": {},
   "source": [
    "Данный ноутбук обрабатывает train.csv и выгружает целевую таблицу inference_baseline с true_label и prediction_score в Платформенную БД."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b611bd-c27e-4152-b181-0693607bba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary==\"2.9.9\" boto3==\"1.35.0\" pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853ca8f-f453-42ec-8b91-0c8364975408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import tempfile\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import tempfile\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3245bf-d22e-42cd-8daf-df23c6620cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_s3_client():\n",
    "    project_s3 = boto3.resource(\n",
    "        \"s3\",\n",
    "        endpoint_url=os.environ.get(\"FEAST_S3_ENDPOINT_URL\"),\n",
    "        aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"), \n",
    "        aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "    \n",
    "    bucket_name = os.environ.get(\"S3_BUCKET\")\n",
    "    print(f\"Бакет: {bucket_name}\")\n",
    "    \n",
    "    return project_s3, bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0a8ca-47cb-42c4-9ae9-13f3f72b3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(password):\n",
    "    DATABASE_USER = \"vectoradmin\"\n",
    "    DATABASE_PASSWORD = password\n",
    "    DATABASE_HOST = os.environ.get(\"PGVECTOR_HOST_NAME\")\n",
    "    DATABASE_PORT = 5432\n",
    "    DATABASE_DBNAME = os.environ.get(\"PGVECTOR_DB_NAME\")\n",
    "    \n",
    "    connection_string = f\"postgresql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_DBNAME}\"\n",
    "    \n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Проверка подключения\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            print(\"Подключение к БД установлено\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка подключения к БД: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5ed21-2c86-4d23-b95a-f7a2948f5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_df_to_s3_parquet(project_s3, df, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Загружает DataFrame в S3 как Parquet файл\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        df: DataFrame для сохранения\n",
    "        bucket_name: Имя S3 бакета\n",
    "        s3_key: Ключ (путь) для сохранения файла в S3\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as tmp_file:\n",
    "        df.to_parquet(tmp_file.name, index=False)\n",
    "        \n",
    "        project_s3.Bucket(bucket_name).upload_file(tmp_file.name, s3_key)\n",
    "\n",
    "        os.unlink(tmp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06504a8d-ea76-4c87-9ec0-2714c8b445da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_read_first_parquet(project_s3, bucket_name, folder_path=\"\"):\n",
    "    \"\"\"\n",
    "    Находит и читает первый parquet файл в указанной папке S3\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        bucket_name: Имя S3 бакета\n",
    "        folder_path: Путь к папке в S3\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame с данными из первого найденного parquet файла\n",
    "    \"\"\"\n",
    "    if folder_path and not folder_path.endswith('/'):\n",
    "        folder_path += '/'\n",
    "  \n",
    "    objects = list(project_s3.Bucket(bucket_name).objects.filter(Prefix=folder_path))\n",
    "    \n",
    "    parquet_files = [obj for obj in objects if obj.key.endswith('.parquet')]\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"Parquet файлы не найдены в s3://{bucket_name}/{folder_path}\")\n",
    "    \n",
    "    # Берем первый найденный parquet файл\n",
    "    first_parquet = parquet_files[0]\n",
    "    s3_key = first_parquet.key\n",
    "    \n",
    "    print(f\"Найден файл: {s3_key}\")\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as tmp_file:\n",
    "        project_s3.Bucket(bucket_name).download_file(s3_key, tmp_file.name)\n",
    "        df = pd.read_parquet(tmp_file.name)\n",
    "        os.unlink(tmp_file.name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42778bb9-48ff-4f33-9b02-7605f3ec2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_data(project_s3, train_csv_path, bucket_name):\n",
    "    \"\"\"\n",
    "    Обрабатывает train.csv файл для создания baseline данных\n",
    "    \n",
    "    Шаги:\n",
    "    1. Загрузка данных из CSV\n",
    "    2. Приведение признаков к целевому виду\n",
    "    3. Сохранение метаданных для последующего объединения\n",
    "    4. Сохранение features в S3 для батч-инференса\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        train_csv_path: Путь к train.csv файлу\n",
    "        bucket_name: Имя S3 бакета\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    print(\"Исходные данные train:\")\n",
    "    print(f\"Размер: {df.shape}\")\n",
    "    print(f\"Колонки: {list(df.columns)}\")\n",
    "    \n",
    "    df['app_date'] = pd.to_datetime(df['app_date'], format='%d%b%Y')\n",
    "    metadata_cols = df[['client_id', 'app_date', 'default']].copy()\n",
    "    \n",
    "    # Применяем преобразования\n",
    "\n",
    "    df['age'] = np.log(df['age'] + 1)\n",
    "    df['decline_app_cnt'] = np.log(df['decline_app_cnt'] + 1)\n",
    "    df['bki_request_cnt'] = np.log(df['bki_request_cnt'] + 1)\n",
    "    df['income'] = np.log(df['income'] + 1)\n",
    "\n",
    "    df['education'] = df['education'].fillna('SCH')\n",
    "    \n",
    "    start = df.app_date.min()\n",
    "    df['days'] = (df.app_date - start).dt.days.astype('int')\n",
    "\n",
    "    bin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in bin_cols:\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "    cat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\n",
    "    df = pd.get_dummies(df, prefix=cat_cols, columns=cat_cols, dtype=int)\n",
    "    \n",
    "    # Удаляем мета-колонки для батч-версии\n",
    "    columns_to_drop = ['app_date', 'client_id', 'default']\n",
    "    df_for_batch = df.drop(columns_to_drop, axis=1, errors='ignore')\n",
    "    \n",
    "    print(\"После преобразований:\")\n",
    "    print(f\"Для batch инференса: {df_for_batch.shape}\")\n",
    "    print(f\"Колонки для batch: {list(df_for_batch.columns)}\")\n",
    "    \n",
    "    upload_df_to_s3_parquet(project_s3, df_for_batch, bucket_name, 'baseline_features.parquet')\n",
    "    \n",
    "    print(f\"Train данные обработаны. Batch features сохранены в S3\")\n",
    "    \n",
    "    return df_for_batch, metadata_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80529dc9-f8ab-4b5c-8d5e-58707bbc0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_baseline(project_s3, bucket_name, predictions_s3_key, original_metadata):\n",
    "    \"\"\"\n",
    "    Загружает baseline предсказания из S3 и объединяет с оригинальными метаданными\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        bucket_name: Имя S3 бакета\n",
    "        predictions_s3_key: Путь к папке с предсказаниями в S3\n",
    "        original_metadata: DataFrame с метаданными (client_id, app_date, default)\n",
    "        \n",
    "    \"\"\"\n",
    "    df_batch = find_and_read_first_parquet(project_s3, bucket_name, predictions_s3_key)\n",
    "\n",
    "    df_batch_reset = df_batch.reset_index(drop=True)\n",
    "    metadata_reset = original_metadata.reset_index(drop=True)\n",
    "    \n",
    "    df_combined = pd.concat([metadata_reset, df_batch_reset], axis=1)\n",
    "    \n",
    "    # Переименовываем колонку default в true_label\n",
    "    if 'default' in df_combined.columns:\n",
    "        df_combined = df_combined.rename(columns={'default': 'true_label'})\n",
    "        print(\"Колонка 'default' переименована в 'true_label'\")\n",
    "    else:\n",
    "        print(\"Колонка 'default' не найдена для переименования\")\n",
    "\n",
    "    df_combined['app_date'] = pd.to_datetime(df_combined['app_date'])\n",
    "    df_combined['app_date'] = df_combined['app_date'].apply(lambda x: x.replace(year=2024))\n",
    "    \n",
    "    print(f\"Данные объединены: {df_combined.shape}\")\n",
    "    print(f\"Колонки итоговые: {list(df_combined.columns)}\")\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e49152-a15a-4c1f-8747-6169b0b9fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "def load_predictions_to_db(df, table_name, vectoradmin_password):\n",
    "    \"\"\"\n",
    "    Загружает DataFrame с предсказаниями в PostgreSQL базу данных\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame для загрузки\n",
    "        table_name: Имя таблицы в БД\n",
    "        vectoradmin_password: Пароль пользователя БД\n",
    "    \"\"\"\n",
    "    db_connection = create_db_connection(vectoradmin_password)\n",
    "    \n",
    "    df.to_sql(table_name, db_connection, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f\"Создана/обновлена таблица '{table_name}' с {len(df)} записями\")\n",
    "    \n",
    "    # Проверяем, что таблица создана и содержит данные\n",
    "    with db_connection.connect() as conn:\n",
    "        result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name}\"))\n",
    "        count = result.scalar()\n",
    "        print(f\"Проверка: в таблице {table_name} {count} записей\")\n",
    "        \n",
    "        result = conn.execute(text(f\"SELECT * FROM {table_name} LIMIT 3\"))\n",
    "        print(f\"Пример записей:\")\n",
    "        for row in result:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fde40-f8f6-4b35-9aa3-16450b522641",
   "metadata": {},
   "source": [
    "### **MAIN PIPELINE**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0648b4-cc32-4a8f-85c4-e7d844c70250",
   "metadata": {},
   "source": [
    "Основной пайплайн для обработки **baseline** данных<br>\n",
    "Выполнять последовательно все шаги:<br>\n",
    "1. Настройка подключений<br>\n",
    "2. Обработка train.csv<br>\n",
    "3. **(Ручной шаг) Запуск батч-сервиса на baseline_features.parquet**<br>\n",
    "4. Загрузка и обработка предсказаний<br>\n",
    "5. **Перед следующим шагом необходимо посмотреть пароль для бд пгвектора и ввести параметр**<br>\n",
    "6. Загрузка в БД"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb975a-b4ee-43f8-81c8-7d0004c86440",
   "metadata": {},
   "source": [
    "### **Настройка подключений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e629021-0373-4d34-8311-df31bdc38acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_s3, bucket_name = setup_s3_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eae45a-0c57-44a4-b9b1-0bf79b8a00c7",
   "metadata": {},
   "source": [
    "### **Обработка train данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9932de7-6278-41aa-a3dc-4dd6d53b5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data, metadata_cols = process_train_data(\n",
    "        project_s3, \n",
    "        \"data/train.csv\", \n",
    "        bucket_name\n",
    "    )\n",
    "print(\"РУЧНОЙ ШАГ: ЗАПУСТИТЕ БАТЧ-СЕРВИС\")\n",
    "print(\"Используйте baseline_features.parquet из S3 для инференса\")\n",
    "print(f\"INPUT_DATA: s3a://{bucket_name}/baseline_features.parquet\")\n",
    "print(f\"OUTPUT_DIRECTORY: s3a://{bucket_name}/inference_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c54fba-09d2-45be-b45c-bcc241361f2f",
   "metadata": {},
   "source": [
    "### **Загрузка baseline предсказаний**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7cda3-7f16-4b01-9f93-b301f3049ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_combined = load_and_combine_baseline(\n",
    "        project_s3, \n",
    "        bucket_name, \n",
    "        \"inference_baseline/\", \n",
    "        metadata_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc618d6b-f069-4791-bbc8-75151a5e4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10991ab9-396d-45d1-a888-adfe126ab6c1",
   "metadata": {},
   "source": [
    "### **Загрузка в базу данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8bc85-30e6-44e1-90b8-8be9853241c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_predictions_to_db(df_combined, \"inference_baseline\", \"vectoradmin_password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e56f5-a631-4efd-bc74-3f3f215f1dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
