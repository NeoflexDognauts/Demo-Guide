{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65f30d5-afc0-470b-95da-20639d68809e",
   "metadata": {},
   "source": [
    "### **PIPELINE ДЛЯ PRIMARY ДАННЫХ (TEST.CSV)**  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07130dd2-35fb-4a4f-bc2c-e0138c4a12d0",
   "metadata": {},
   "source": [
    "Данный ноутбук обрабатывает test.csv и выгружает целевую таблицу inference_primary с true_label и prediction_score в Платформенную БД."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0846a87-7e3f-49ff-bad7-eb8337236e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary==\"2.9.9\" boto3==\"1.35.0\" pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1d06c-7ed3-4887-a0a3-b7a18b4ff6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import tempfile\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f38b14-8944-48bb-aa0a-8675d6c9f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_s3_client():\n",
    "    project_s3 = boto3.resource(\n",
    "        \"s3\",\n",
    "        endpoint_url=os.environ.get(\"FEAST_S3_ENDPOINT_URL\"),\n",
    "        aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"), \n",
    "        aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "    \n",
    "    bucket_name = os.environ.get(\"S3_BUCKET\")\n",
    "    print(f\"Бакет: {bucket_name}\")\n",
    "    \n",
    "    return project_s3, bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76449fd0-2b71-456b-8f6b-747cc5b9592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection(password):\n",
    "    DATABASE_USER = \"vectoradmin\"\n",
    "    DATABASE_PASSWORD = password\n",
    "    DATABASE_HOST = os.environ.get(\"PGVECTOR_HOST_NAME\")\n",
    "    DATABASE_PORT = 5432\n",
    "    DATABASE_DBNAME = os.environ.get(\"PGVECTOR_DB_NAME\")\n",
    "    \n",
    "    connection_string = f\"postgresql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_DBNAME}\"\n",
    "    \n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Проверка подключения\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            print(\"Подключение к БД установлено\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка подключения к БД: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfb552-e393-497c-8371-ef37e51b5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_df_to_s3_parquet(project_s3, df, bucket_name, s3_key):\n",
    "    \"\"\"\n",
    "    Загружает DataFrame в S3 как Parquet файл\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        df: DataFrame для сохранения\n",
    "        bucket_name: Имя S3 бакета\n",
    "        s3_key: Ключ (путь) для сохранения файла в S3\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as tmp_file:\n",
    "        df.to_parquet(tmp_file.name, index=False)\n",
    "        \n",
    "        project_s3.Bucket(bucket_name).upload_file(tmp_file.name, s3_key)\n",
    "\n",
    "        os.unlink(tmp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e0c0a-ac5b-4168-95b5-d810c0aec894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_read_first_parquet(project_s3, bucket_name, folder_path=\"\"):\n",
    "    \"\"\"\n",
    "    Находит и читает parquet файл в указанной папке S3\n",
    "    (по дате последнего изменения)\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        bucket_name: Имя S3 бакета\n",
    "        folder_path: Путь к папке в S3\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame с данными из самого нового parquet файла\n",
    "    \"\"\"\n",
    "    if folder_path and not folder_path.endswith('/'):\n",
    "        folder_path += '/'\n",
    "    \n",
    "    # Получаем список объектов\n",
    "    objects = list(project_s3.Bucket(bucket_name).objects.filter(Prefix=folder_path))\n",
    "    \n",
    "    # Фильтруем только parquet файлы\n",
    "    parquet_files = [obj for obj in objects if obj.key.endswith('.parquet')]\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"arquet файлы не найдены в s3://{bucket_name}/{folder_path}\")\n",
    "    \n",
    "    parquet_files_sorted = sorted(\n",
    "        parquet_files, \n",
    "        key=lambda x: x.last_modified, \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    latest_parquet = parquet_files_sorted[0]\n",
    "    s3_key = latest_parquet.key\n",
    "    \n",
    "    print(f\"Найден файл: {s3_key}\")\n",
    "    print(f\"Дата изменения: {latest_parquet.last_modified}\")\n",
    "    # Читаем файл\n",
    "    with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as tmp_file:\n",
    "        project_s3.Bucket(bucket_name).download_file(s3_key, tmp_file.name)\n",
    "        df = pd.read_parquet(tmp_file.name)\n",
    "        os.unlink(tmp_file.name)\n",
    "    \n",
    "    print(f\"Загружен файл из S3: {s3_key}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df0e68-a554-42f2-917b-77cddb10423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(project_s3, test_csv_path, bucket_name):\n",
    "    \"\"\"\n",
    "    Обрабатывает test.csv файл для создания primary данных\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        test_csv_path: Путь к test.csv файлу\n",
    "        bucket_name: Имя S3 бакета\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    print(\"Исходные данные test:\")\n",
    "    print(f\"Размер: {df.shape}\")\n",
    "    print(f\"Колонки: {list(df.columns)}\")\n",
    "    \n",
    "    df['app_date'] = pd.to_datetime(df['app_date'], format='%d%b%Y')\n",
    "    metadata_cols = df[['client_id', 'app_date']].copy()\n",
    "    \n",
    "    # Применяем преобразования\n",
    "    \n",
    "    df['age'] = np.log(df['age'] + 1)\n",
    "    df['decline_app_cnt'] = np.log(df['decline_app_cnt'] + 1)\n",
    "    df['bki_request_cnt'] = np.log(df['bki_request_cnt'] + 1)\n",
    "    df['income'] = np.log(df['income'] + 1)\n",
    "    \n",
    "    df['education'] = df['education'].fillna('SCH')\n",
    "    \n",
    "    start = df.app_date.min()\n",
    "    df['days'] = (df.app_date - start).dt.days.astype('int')\n",
    "    \n",
    "    bin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in bin_cols:\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "    \n",
    "    cat_cols = ['education', 'region_rating', 'home_address', 'work_address', 'sna', 'first_time']\n",
    "    df = pd.get_dummies(df, prefix=cat_cols, columns=cat_cols, dtype=int)\n",
    "    \n",
    "    # Удаляем мета-колонки для батч-версии\n",
    "    columns_to_drop = ['app_date', 'client_id']\n",
    "    df_for_batch = df.drop(columns_to_drop, axis=1, errors='ignore')\n",
    "    \n",
    "    print(\"После преобразований:\")\n",
    "    print(f\"Для batch инференса: {df_for_batch.shape}\")\n",
    "    print(f\"Колонки для batch: {list(df_for_batch.columns)}\")\n",
    "    \n",
    "    upload_df_to_s3_parquet(project_s3, df_for_batch, bucket_name, 'primary_features.parquet')\n",
    "    \n",
    "    print(f\"Test данные обработаны. Primary features сохранены в S3\")\n",
    "    \n",
    "    return df_for_batch, metadata_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c8c7f-2b46-4715-9f6d-a0e2339098b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_primary(predictions_s3_key, original_metadata):\n",
    "    \"\"\"\n",
    "    Загружает DataFrame с предсказаниями в PostgreSQL базу данных с отклонением ±5% от prediction_score\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        bucket_name: Имя S3 бакета\n",
    "        df: DataFrame для загрузки\n",
    "        table_name: Имя таблицы в БД\n",
    "    \"\"\"\n",
    "    df_batch = find_and_read_first_parquet(project_s3, bucket_name, predictions_s3_key)\n",
    "    \n",
    "    df_combined = pd.concat([\n",
    "        original_metadata.reset_index(drop=True), \n",
    "        df_batch.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Генерируем true_label с отклонением ±40%\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    if 'prediction_score' in df_combined.columns:\n",
    "        deviation = 0.4\n",
    "        base_proba = df_combined['prediction_score'].values\n",
    "        \n",
    "        noise = np.random.uniform(-deviation, deviation, size=len(df_combined))\n",
    "        noisy_proba = np.clip(base_proba + noise, 0, 1)\n",
    "        \n",
    "        df_combined['true_label'] = (np.random.random(len(df_combined)) < noisy_proba).astype(int)\n",
    "    else:\n",
    "        df_combined['app_date'] = pd.to_datetime(df_combined['app_date'], errors='coerce')\n",
    "    \n",
    "    \n",
    "    df_combined['app_date'] = pd.to_datetime(df_combined['app_date'])\n",
    "    df_combined['app_date'] = df_combined['app_date'].apply(lambda x: x.replace(year=2024))\n",
    "    \n",
    "    print(f\"Данные объединены: {df_combined.shape}\")\n",
    "    print(f\"   Дефолтов: {df_combined['true_label'].sum()}/{len(df_combined)} ({df_combined['true_label'].mean():.1%})\")\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797a008-eb4a-4016-af21-390919d43fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_primary_test(predictions_s3_key, original_metadata, deviation_percent=15.0):\n",
    "    \"\"\"\n",
    "    Загружает DataFrame с предсказаниями и генерирует true_label так,\n",
    "    чтобы метрики были с отклонением -deviation_percent% от baseline-значений.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_batch = find_and_read_first_parquet(project_s3, bucket_name, predictions_s3_key)\n",
    "    df_combined = pd.concat([\n",
    "        original_metadata.reset_index(drop=True), \n",
    "        df_batch.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Baseline метрики\n",
    "    baseline_metrics = {\n",
    "        'accuracy': 0.670528,\n",
    "        'precision': 0.226289,\n",
    "        'recall': 0.686918,\n",
    "        'f1': 0.340431\n",
    "    }\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    if 'prediction_score' in df_combined.columns:\n",
    "        base_proba = df_combined['prediction_score'].values\n",
    "        y_pred = (base_proba > 0.5).astype(int)\n",
    "        n = len(y_pred)\n",
    "        \n",
    "        true_label = y_pred.copy()\n",
    "        \n",
    "        # 2. Ухудшаем accuracy на deviation_percent%\n",
    "        target_acc = baseline_metrics['accuracy'] * (1 - deviation_percent/100)\n",
    "        current_acc = np.mean(true_label == y_pred)\n",
    "        \n",
    "        # Сколько нужно изменить предсказаний для достижения target_acc\n",
    "        n_to_change = int(abs(current_acc - target_acc) * n)\n",
    "        \n",
    "        if current_acc > target_acc:  # Нужно ухудшить accuracy\n",
    "            # Находим индексы где предсказание правильное\n",
    "            correct_idx = np.where(true_label == y_pred)[0]\n",
    "            # Случайно выбираем часть правильных и меняем на противоположные\n",
    "            change_idx = np.random.choice(correct_idx, size=n_to_change, replace=False)\n",
    "            true_label[change_idx] = 1 - true_label[change_idx]\n",
    "        \n",
    "        # 3. Ухудшаем precision на deviation_percent%\n",
    "        target_prec = baseline_metrics['precision'] * (1 - deviation_percent/100)\n",
    "        pred_pos_idx = np.where(y_pred == 1)[0]\n",
    "        \n",
    "        if len(pred_pos_idx) > 0:\n",
    "            # Вычисляем сколько должно быть TP\n",
    "            target_TP = int(target_prec * len(pred_pos_idx))\n",
    "            current_TP = np.sum((y_pred[pred_pos_idx] == 1) & (true_label[pred_pos_idx] == 1))\n",
    "            \n",
    "            if current_TP > target_TP:\n",
    "                # Находим TP которые нужно превратить в FP\n",
    "                tp_idx = pred_pos_idx[(y_pred[pred_pos_idx] == 1) & (true_label[pred_pos_idx] == 1)]\n",
    "                n_to_change = current_TP - target_TP\n",
    "                change_idx = np.random.choice(tp_idx, size=min(n_to_change, len(tp_idx)), replace=False)\n",
    "                true_label[change_idx] = 0\n",
    "        \n",
    "        df_combined['true_label'] = true_label\n",
    "    else:\n",
    "        df_combined['true_label'] = np.random.choice([0, 1], size=len(df_combined))\n",
    "    \n",
    "    # Проверяем результат\n",
    "    y_true = df_combined['true_label'].values\n",
    "    y_pred = (df_combined['prediction_score'].values > 0.5).astype(int) if 'prediction_score' in df_combined.columns else None\n",
    "    \n",
    "    if y_pred is not None:\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0385b9e-32e4-4250-b7f4-e6a016183a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions_to_db(project_s3, bucket_name, df, table_name, vectoradmin_password):\n",
    "    \"\"\"\n",
    "    Загружает DataFrame с предсказаниями в PostgreSQL базу данных\n",
    "    \n",
    "    Args:\n",
    "        project_s3: S3 клиент\n",
    "        bucket_name: Имя S3 бакета\n",
    "        df: DataFrame для загрузки\n",
    "        table_name: Имя таблицы в БД\n",
    "    \"\"\"\n",
    "    db_connection = create_db_connection(vectoradmin_password)\n",
    "    \n",
    "    df.to_sql(table_name, db_connection, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f\"Создана/обновлена таблица '{table_name}' с {len(df)} записями\")\n",
    "    \n",
    "    # with db_connection.connect() as conn:\n",
    "    #     result = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    #     count = result.scalar()\n",
    "    #     print(f\"Проверка: в таблице {table_name} {count} записей\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a398273-f8e1-4da7-a643-437c22a6a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_degraded_window(predictions_s3_key, original_metadata, vectoradmin_password, degradation=0.10, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Создает деградированные данные и добавляет их в таблицу inference_primary через APPEND\n",
    "    \"\"\"\n",
    "    # Загружаем данные\n",
    "    df_batch = find_and_read_first_parquet(project_s3, bucket_name, predictions_s3_key)\n",
    "    df_combined = pd.concat([original_metadata.reset_index(drop=True), df_batch.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Baseline метрики\n",
    "    baseline = {\n",
    "        'accuracy': 0.670528,\n",
    "        'precision': 0.226289, \n",
    "        'recall': 0.686918,\n",
    "        'f1': 0.340431\n",
    "    }\n",
    "    \n",
    "    # Целевые метрики\n",
    "    target = {k: v * (1 - degradation) for k, v in baseline.items()}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    if 'prediction_score' in df_combined.columns:\n",
    "        proba = df_combined['prediction_score'].values\n",
    "        y_pred = (proba > 0.5).astype(int)\n",
    "        n = len(y_pred)\n",
    "        \n",
    "        # Начинаем с правильных предсказаний\n",
    "        y_true = y_pred.copy()\n",
    "        \n",
    "        # Добавляем ошибки для достижения target accuracy\n",
    "        target_correct = int(n * target['accuracy'])\n",
    "        errors_needed = n - target_correct\n",
    "        \n",
    "        if errors_needed > 0:\n",
    "            error_indices = np.random.choice(n, size=errors_needed, replace=False)\n",
    "            for idx in error_indices:\n",
    "                y_true[idx] = 1 - y_true[idx]\n",
    "        \n",
    "        df_combined['true_label'] = y_true\n",
    "        \n",
    "        # Проверяем метрики\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        achieved = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "        }\n",
    "        \n",
    "#         print(f\"Target metrics ({degradation*100:.0f}% degradation):\")\n",
    "#         for metric, value in target.items():\n",
    "#             print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "#         print(\"\\nAchieved metrics:\")\n",
    "#         for metric, value in achieved.items():\n",
    "#             print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "#         print(\"\\nDeviation from baseline (%):\")\n",
    "#         for metric in baseline:\n",
    "#             deviation = ((achieved[metric] - baseline[metric]) / baseline[metric]) * 100\n",
    "#             print(f\"  {metric}: {deviation:.1f}%\")\n",
    "            \n",
    "#             if abs(deviation) > 15:\n",
    "#                 print(f\"THRESHOLD TRIGGERED! > 15%\")\n",
    "    \n",
    "    else:\n",
    "        df_combined['true_label'] = np.random.choice([0, 1], size=len(df_combined), p=[0.85, 0.15])\n",
    "    \n",
    "    db_connection = create_db_connection(vectoradmin_password)\n",
    "    \n",
    "    df_combined['app_date'] = pd.to_datetime(df_combined['app_date'])\n",
    "    df_combined['app_date'] = df_combined['app_date'].apply(lambda x: x.replace(year=2025, month=11, day=30))\n",
    "    \n",
    "    # Добавляем в БД\n",
    "    try:\n",
    "        df_combined.to_sql('inference_primary', db_connection, if_exists='append', index=False)\n",
    "        print(\"Данные успешно добавлены!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "    finally:\n",
    "        db_connection.dispose()\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5a492-f792-4bd0-a10a-aea3d732ba24",
   "metadata": {},
   "source": [
    "### **MAIN PIPELINE**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33b152-32ff-45de-b6af-8602c1798078",
   "metadata": {},
   "source": [
    "Основной пайплайн для обработки **primary** данных<br>\n",
    "Выполнять последовательно все шаги:<br>\n",
    "1. Настройка подключений<br>\n",
    "2. Обработка test.csv<br>\n",
    "3. **(Ручной шаг) Запуск батч-сервиса на primary_features.parquet**<br>\n",
    "4. Загрузка и обработка предсказаний с генерацией synthetic true_label<br>\n",
    "6. **Перед следующим шагом необходимо посмотреть пароль для бд пгвектора и ввести параметр**<br>\n",
    "5. Загрузка в БД<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c729b27-2b84-4dba-9385-95cebf6b628e",
   "metadata": {},
   "source": [
    "### **Настройка подключений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf53039-b9b0-4b06-9df5-20788d3e19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_s3, bucket_name = setup_s3_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f1d5f-b242-4f2f-8081-60387ff89c86",
   "metadata": {},
   "source": [
    "### **Обработка test данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26b3db-41b9-4e84-bc34-429e38b7c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_data, metadata_cols = process_test_data(\n",
    "        project_s3, \n",
    "        \"data/test.csv\", \n",
    "        bucket_name\n",
    "    )\n",
    "print(\"РУЧНОЙ ШАГ: ЗАПУСТИТЕ БАТЧ-СЕРВИС\")\n",
    "print(\"Используйте primary_features.parquet из S3 для инференса\")\n",
    "print(f\"INPUT_DATA: s3a://{bucket_name}/primary_features.parquet\")\n",
    "print(f\"OUTPUT_DIRECTORY: s3a://{bucket_name}/inference_primary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0d452-f04d-4552-bf8d-72c6de332916",
   "metadata": {},
   "source": [
    "### **Загрузка primary предсказаний**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7f41a-2af4-4260-954a-8bcf6e6f8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "df_combined = load_and_combine_primary_test(\"inference_primary/\", metadata_cols, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bd4b3-7aec-49ac-b09d-eaea7228b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46939b-bb5a-4df3-9e8a-a708562e2b7f",
   "metadata": {},
   "source": [
    "### **Загрузка в базу данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0759627-1580-4512-ae57-23a1f29ff693",
   "metadata": {},
   "outputs": [],
   "source": [
    " load_predictions_to_db(project_s3, bucket_name, df_combined, \"inference_primary\", \"vectoradmin_password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cd076-3ad0-45a8-a03e-1cdce9653a36",
   "metadata": {},
   "source": [
    "### **Добавление новых данных в таблицу**\n",
    "**Необходимо посмотреть пароль для бд пгвектора и ввести параметр**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e046a82-e110-4195-b305-554ef4d02b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_window = add_degraded_window(\n",
    "    predictions_s3_key=\"inference_primary/\",\n",
    "    original_metadata=metadata_cols,\n",
    "    degradation=0.1,\n",
    "    vectoradmin_password=\"vectoradmin_password\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8772c71-c5dc-484a-91bd-670ce4a294e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
